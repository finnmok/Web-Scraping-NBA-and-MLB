{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44627439",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12e7ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install fake_useragent\n",
    "!pip install --upgrade fake_useragent\n",
    "!pip3 install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from lxml import etree\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from re import match\n",
    "from fake_useragent import UserAgent\n",
    "import fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapeimports\n",
    "from scrapeimports import get_rot_prox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_ads = get_rot_prox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(ip_ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48120918",
   "metadata": {},
   "source": [
    "# Functions for Grabbing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600aff9f",
   "metadata": {},
   "source": [
    "### Quick Verification That Able to Access (Not Jailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://www.basketball-reference.com/leagues/NBA_{2021}_totals.html#totals_stats'\n",
    "page = requests.get(url, proxies={'http':random.choice(ip_ads)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac77e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://en.wikipedia.org//wiki/Rafer_Alston'\n",
    "page = requests.get(url, proxies={'http':random.choice(ip_ads)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a762487",
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe733c6",
   "metadata": {},
   "source": [
    "### Grab Players and URLS from basketball-reference.com and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5551d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlayers(season_yr):\n",
    "    \n",
    "    \"\"\"\n",
    "    GRABS ALL PLAYERS IN AN NBA SEASON\n",
    "    \n",
    "    input: int season_yr (basketball season year. eg 2021 represents 2020-2021 NBA season)\n",
    "    output: list[str] of NBA Players (format: 'Lastname,Firstname')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #URL for all players in the season\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season_yr}_totals.html#totals_stats'\n",
    "    page = requests.get(url, proxies={'http':random.choice(ip_ads)})\n",
    "    \n",
    "    #Grab the list from the website\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    all_people = soup.find_all('tr',class_='full_table')\n",
    "    \n",
    "    #Go through list and collect each player\n",
    "    nba_players = []\n",
    "    for person in all_people:\n",
    "        nba_players.append(person.find('td')['csk'])\n",
    "        \n",
    "    return nba_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee401f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWikiPage(player_list,index):\n",
    "    \n",
    "    \"\"\"\n",
    "    GET WIKI PAGE FOR PLAYER BASED ON LIST NAME AND INDEX\n",
    "    \n",
    "    input: list[str] getPlayers(), int index\n",
    "    \n",
    "    output: str Wikipedia URL\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create Search Using First/Last Name\n",
    "    global player_first\n",
    "    player_first = player_list[index].split(',')[1]\n",
    "    \n",
    "    global player_last\n",
    "    player_last = player_list[index].split(',')[0]\n",
    "    \n",
    "    search_url = f'https://en.wikipedia.org/w/index.php?search=nba+{player_first}+{player_last}&title=Special:Search&profile=advanced&fulltext=1&ns0=1'\n",
    "    \n",
    "    search_page = requests.get(search_url, proxies={'http':random.choice(ip_ads)})\n",
    "    soup = BeautifulSoup(search_page.text, 'lxml')\n",
    "    \n",
    "    for item in soup.find_all('div',class_ = 'mw-search-result-heading'):\n",
    "        if player_last.lower() in item.find('a')['title'].lower():\n",
    "            if player_first.lower() in item.find('a')['title'].lower():\n",
    "                return 'https://en.wikipedia.org/wiki/' + item.find('a')['title'].replace(' ','_')\n",
    "    \n",
    "    return 'https://en.wikipedia.org/wiki/' + soup.find_all('div',class_ = 'mw-search-result-heading')[0].find('a')['title'].replace(' ','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22377609",
   "metadata": {},
   "source": [
    "### Collect Personal, Career, History, and Highlight Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPersonalInfo(url):\n",
    "\n",
    "    \"\"\"\n",
    "    GET INFORMATION FOR A PLAYER\n",
    "    \n",
    "    input:  str url (Player URL to scrape)\n",
    "    \n",
    "    output: list born, str nationality, str height, str weight\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Go to Wiki Page\n",
    "    wiki_page = requests.get(url, proxies={'http':random.choice(ip_ads)})\n",
    "    soup = BeautifulSoup(wiki_page.text, 'lxml')\n",
    "    \n",
    "    #Go to Wiki Player information box\n",
    "    #Example box here: \n",
    "    infobox = soup.find('table',class_='infobox vcard')\n",
    "    if infobox == None:\n",
    "        infobox = soup.find('table',class_='infobox ib-baseball-bio vcard')\n",
    "    if infobox == None:\n",
    "        return None, None, None, None\n",
    "    all_facts = infobox.find_all('tr')\n",
    "    \n",
    "\n",
    "    born,nationality,height,weight = None,None,None,None\n",
    "    \n",
    "    for item in all_facts:\n",
    "\n",
    "        #Get Birthday and Birthplace\n",
    "        if 'Born' in item.get_text():\n",
    "            txt = item.get_text().replace('\\xa0',' ').replace('Born ','')\n",
    "            pattern = re.compile(r\"\\(age \\d\\d\\)\", re.IGNORECASE)\n",
    "            born = re.split(pattern,txt)\n",
    "\n",
    "        #Get Nationality\n",
    "        if 'Nationality' in item.get_text():\n",
    "            nationality = item.get_text().replace('\\xa0',' ').replace(\"Nationality\",'')\n",
    "\n",
    "        #Get Listed Height    \n",
    "        if 'height' in item.get_text():\n",
    "            height = item.get_text().replace('\\xa0',' ').replace('Listed height','')\n",
    "\n",
    "        #Get Current Weight\n",
    "        if 'weight' in item.get_text():\n",
    "            weight = item.get_text().replace('\\xa0',' ').replace('Listed weight','')\n",
    "\n",
    "    return born, nationality, height, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a527421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCareer(url):\n",
    "    \n",
    "    \"\"\"\n",
    "    GET INFORMATION FOR A PLAYER\n",
    "    \n",
    "    input:  str url (Player URL to scrape)\n",
    "    \n",
    "    output: str highschool, str college, str draft, str careeryrs\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Go to Wiki Page\n",
    "    wiki_page = requests.get(url, proxies={'http':random.choice(ip_ads)})\n",
    "    soup = BeautifulSoup(wiki_page.text, 'lxml')\n",
    "    \n",
    "    #Go to Wiki Player information box\n",
    "    #Example box here: \n",
    "    infobox = soup.find('table',class_='infobox vcard')\n",
    "    if infobox == None:\n",
    "        infobox = soup.find('table',class_='infobox ib-baseball-bio vcard')\n",
    "    if infobox == None:\n",
    "        return None, None, None, None\n",
    "    all_facts = infobox.find_all('tr')\n",
    "    \n",
    "    highschool,college,draft,careeryrs = None,None,None,None\n",
    "    \n",
    "    for item in all_facts:\n",
    "\n",
    "        #Get High School\n",
    "        if 'High school' in item.get_text():\n",
    "            highschool = item.get_text().replace('High school','').split('\\n')\n",
    "            highschool = list(filter(lambda x: x!='',highschool))\n",
    "\n",
    "            if len(highschool) == 1:\n",
    "                highschool = highschool[0]\n",
    "\n",
    "        #Get College\n",
    "        if 'College' in item.get_text():\n",
    "            college = item.get_text().replace('College','').split('\\n')\n",
    "            college = list(filter(lambda x: x!='',college))\n",
    "\n",
    "            if len(college) == 1:\n",
    "                college = college[0]\n",
    "\n",
    "        #Get Draft Information (Year, Round, Pick Overall)        \n",
    "        if 'NBA draft' in item.get_text():\n",
    "            draft = item.get_text().replace('NBA draft','').split(' / ')[0]\n",
    "\n",
    "        #Get Years for their Career\n",
    "        if 'Playing career' in item.get_text():\n",
    "            careeryrs = item.get_text().replace(\"Playing career\",'')\n",
    "\n",
    "    return highschool, college, draft, careeryrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dadf6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHistory(url):\n",
    "   \n",
    "    \"\"\"\n",
    "    GET INFORMATION FOR A PLAYER\n",
    "    \n",
    "    input:  str url (Player URL to scrape)\n",
    "    \n",
    "    output: list careerhistory\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Go to Wiki Page\n",
    "    wiki_page = requests.get(url, proxies={'http':random.choice(ip_ads)})\n",
    "    soup = BeautifulSoup(wiki_page.text, 'lxml')\n",
    "    \n",
    "    #Go to Wiki Player information box\n",
    "    #Example box here: \n",
    "    infobox = soup.find('table',class_='infobox vcard')\n",
    "    if infobox == None:\n",
    "        infobox = soup.find('table',class_='infobox ib-baseball-bio vcard')\n",
    "    if infobox == None:\n",
    "        return None, None, None, None\n",
    "    all_facts = infobox.find_all('tr')\n",
    "    \n",
    "    careerhistory = None\n",
    "    atag,btag = None,None\n",
    "    \n",
    "    for item in all_facts:\n",
    "        \n",
    "\n",
    "        #Find Indexes to Get Their Career Years\n",
    "        if \"Career history\" in item.get_text():\n",
    "            atag = item\n",
    "\n",
    "        if \"Career highlights and awards\" in item.get_text():\n",
    "            btag = item\n",
    "        elif \"Career NBA  statistics\" in item.get_text():\n",
    "            btag = item\n",
    "\n",
    "        if (atag != None) and (btag != None):\n",
    "            a = all_facts.index(atag) + 1\n",
    "            b = all_facts.index(btag)\n",
    "            careerhistory = []\n",
    "\n",
    "            for team in all_facts[a:b]:\n",
    "                careerhistory.append(team.get_text())\n",
    "            careerhistory = list(filter(lambda x: x!='\\n',careerhistory))\n",
    "\n",
    "    return careerhistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHighlights(url):\n",
    "      \n",
    "\n",
    "    \"\"\"\n",
    "    GET INFORMATION FOR A PLAYER\n",
    "    \n",
    "    input:  str url (Player URL to scrape)\n",
    "    \n",
    "    output: list highlights\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Go to Wiki Page\n",
    "    wiki_page = requests.get(url, proxies={'http':random.choice(ip_ads)})\n",
    "    soup = BeautifulSoup(wiki_page.text, 'lxml')\n",
    "    \n",
    "    #Go to Wiki Player information box\n",
    "    #Example box here: \n",
    "    infobox = soup.find('table',class_='infobox vcard')\n",
    "    if infobox == None:\n",
    "        infobox = soup.find('table',class_='infobox ib-baseball-bio vcard')\n",
    "    if infobox == None:\n",
    "        return None, None, None, None\n",
    "    all_facts = infobox.find_all('tr')\n",
    "    \n",
    "    careerhighlights = None\n",
    "    ctag,dtag,etag = None,None,None\n",
    "    \n",
    "    for item in all_facts:\n",
    "\n",
    "        #Get Information on Career Highlights and Awards\n",
    "        if \"Career highlights and awards\" in item.get_text():\n",
    "            ctag = item\n",
    "        \n",
    "        if \"Career NBA  statistics\" in item.get_text():\n",
    "            dtag = item\n",
    "        elif 'at NBA.com' in item.get_text():\n",
    "            dtag = item\n",
    "        \n",
    "        if (ctag != None) and (dtag != None):    \n",
    "            c = all_facts.index(ctag) + 1\n",
    "            d = all_facts.index(dtag)\n",
    "            careerhighlights = []\n",
    "\n",
    "            for award in all_facts[c:d]:\n",
    "                careerhighlights.append(award.get_text())\n",
    "            #highlights = list(filter(lambda x: x!='',careerhighlights[0].split('\\n')))\n",
    "    \n",
    "    #return highlights      \n",
    "    return careerhighlights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89d99b",
   "metadata": {},
   "source": [
    "# Grabbing Seasons 1999-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc642d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe to store all information\n",
    "\n",
    "nba_stats_wiki = pd.DataFrame(columns=[\"Year\",\"Name\",\"Born\",\"Nationality\",\"Height\",\"Weight\",\n",
    "                                       \"Highschool\",\"College\",\"Draft\",\"CareerYears\",\n",
    "                                       \"History\",\"Awards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab All Information Using Functions Created Earlier (eg. getPersonalInfo and getHighlights)\n",
    "\n",
    "import time\n",
    "\n",
    "def getSeasonData(year):\n",
    "    startTime = time.time()\n",
    "\n",
    "    nba_yr = getPlayers(year)\n",
    "    \n",
    "    for n in range(len(nba_yr)):\n",
    "        if n % 10 == 0:\n",
    "            print(year,\": \", n+1, \" out of \",len(nba_yr) )\n",
    "        url = getWikiPage(nba_yr,n)\n",
    "\n",
    "        born, nationality, height, weight = getPersonalInfo(url)\n",
    "        highschool, college, draft, careeryrs = getCareer(url)\n",
    "        careerhistory = getHistory(url)\n",
    "        highlights = getHighlights(url)\n",
    "\n",
    "        global nba_stats_wiki\n",
    "        nba_stats_wiki = nba_stats_wiki.append({\"Year\":year,\"Name\":nba_yr[n],\"Born\":born,\"Nationality\":nationality,\n",
    "                                                \"Height\":height,\"Weight\":weight,\"Highschool\":highschool,\n",
    "                                                \"College\":college,\"Draft\":draft,\"CareerYears\":careeryrs,\n",
    "                                                \"History\":careerhistory,\"Awards\":highlights},ignore_index=True)\n",
    "    executionTime = (time.time() - startTime)\n",
    "    print(f'Execution time in seconds for {year} season: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea83219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loop Through Each Year Grabbing Every Player in the Season\n",
    "\n",
    "for yr in range(1999,2024):\n",
    "    getSeasonData(yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82da482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to .csv File\n",
    "\n",
    "nba_stats_wiki.to_csv('C:\\\\Users\\\\finnr\\\\Downloads\\\\Zhao McIntire Research\\\\nba_player_bio_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2306dc",
   "metadata": {},
   "source": [
    "### Max 20 Requests per Minute<br><br> Jail for 1 Hour<br><br>[Scraping Rules for Basketball-References.com](https://www.sports-reference.com/bot-traffic.html) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
