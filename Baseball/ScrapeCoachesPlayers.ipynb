{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d49fb226-c6ef-46d5-9f6a-c40dfc265455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from lxml import etree\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from re import match\n",
    "from fake_useragent import UserAgent\n",
    "import fake_useragent\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9f8fe7d-b924-4093-8eb3-b96d8cf16f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1002dff9-3449-4498-850e-d8b02821c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\finnr\\Downloads\\Zhao McIntire Research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "3001d859-5151-4adf-9113-dbe2df0a8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socketserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c7db082-c435-4552-9dde-ac5a6c85784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapeimports\n",
    "from scrapeimports import get_rot_prox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "bc8a8d3d-c7dd-4fd5-897b-055ac1d91886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'154.202.113.197'"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_ads = get_rot_prox()\n",
    "random.choice(ip_ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a30a5e-5680-4b3b-82cc-0b0018671607",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify If Accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ee7967b-d1be-4b15-9a46-49792e688a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://www.baseball-reference.com/teams/NYM/2021.shtml'\n",
    "page = requests.get(url, proxies={'http':random.choice(ip_ads)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a921f6-84ca-4f04-abc1-729ae13e9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlayers(season_yr):\n",
    "    \n",
    "    \"\"\"\n",
    "    GRABS ALL PLAYERS IN AN MLB SEASON\n",
    "    \n",
    "    input: int season_yr (basketball season year. eg 2021 represents 2020-2021 NBA season)\n",
    "    output: list[str] of NBA Players (format: 'Lastname,Firstname')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #URL for all players in the season\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season_yr}_totals.html#totals_stats'\n",
    "    page = requests.get(url, proxies={'http':random.choice(ip_ads)})\n",
    "    \n",
    "    #Grab the list from the website\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    all_people = soup.find_all('tr',class_='full_table')\n",
    "    \n",
    "    #Go through list and collect each player\n",
    "    nba_players = []\n",
    "    for person in all_people:\n",
    "        nba_players.append(person.find('td')['csk'])\n",
    "        \n",
    "    return nba_players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e1b08-9f10-4dcb-a678-6e6832fd4f74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TeamCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14a86403-44bb-40a0-b023-6a278543a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL for all players in the season\n",
    "url = f'https://www.baseball-reference.com/leagues/majors/2021-standings.shtml'\n",
    "page = requests.get(url, proxies={'http':random.choice(ip_ads)})\n",
    "\n",
    "#Grab the list from the website\n",
    "soup = BeautifulSoup(page.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ec961a66-ac31-4835-9775-2b385975e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teaminits(x):\n",
    "    return x.replace('/teams/','').replace('/2021.shtml','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2eb78492-57be-4524-9ba6-1badeb2aedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = [[teaminits(x.find('th').find('a')['href']) for x in soup.find_all('table')[n].find('tbody').find_all('tr')] for n in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2dcfd718-fd73-4f7c-93ec-52ee25761137",
   "metadata": {},
   "outputs": [],
   "source": [
    "teamcodes = [t for sublist in teams for t in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49708b-3622-470f-ad20-d21abfc07608",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TeamPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "73cab12b-6e29-4409-adb5-0ef2af0306bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "e231cf72-1752-4009-8e68-2c8b32505b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_soup = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "02a10f93-52e4-48aa-b878-cb87bfb99f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_coach_info = pd.DataFrame(columns = [0,1,2,3,4,5,\"Coach\",\"URL\",\"TM\",\"Season\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "22faf59a-0c94-4b8a-9419-8a4525284f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_player_info = pd.DataFrame(columns = range(28))\n",
    "total_player_info[[\"Player\",\"URL\",\"TM\",\"Season\"]] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e69f1619-071e-4293-8534-adf7ca663e6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def get_people_info(teamcode):\n",
    "    \n",
    "    #starting information for link\n",
    "    yr = 2021\n",
    "    url = f'https://www.baseball-reference.com/teams/{teamcode}/{yr}.shtml'\n",
    "    \n",
    "    while (yr != 1998):\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        #Access Page\n",
    "        headers = {\"User-Agent\": ua.random,\n",
    "           \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "           \"Accept-Language\": \"en-US,en;q=0.7\",\n",
    "           \"Accept-Encoding\": \"gzip, deflate\",\n",
    "           \"Referer\": \"https://www.google.com/\",\n",
    "           \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
    "        \n",
    "        #Get Information From Page\n",
    "        global page\n",
    "        global soup\n",
    "        page = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        all_soup.append(soup)\n",
    "\n",
    "        url = 'https://www.baseball-reference.com' + soup.find('a',class_='button2 prev')['href']\n",
    "    \n",
    "\n",
    "        #Coach Convert Commented Table to Soup\n",
    "        coach = BeautifulSoup(\n",
    "            [i for i in soup.find_all(string=lambda text: isinstance(text, Comment)) if 'id=\"coaches\"' in i][0]).find('tbody')\n",
    "\n",
    "        #Grab Coach Names\n",
    "        coaching_staff = [x.get_text() for x in coach.find_all('th')]\n",
    "\n",
    "        #Grab Coach Info Chart\n",
    "        coach_info = np.array([x.get_text() for x in coach.find_all('td')]).reshape(len(coaching_staff),-1)\n",
    "\n",
    "        #Get Links for Each Coach\n",
    "        hrefs_coach = [x.find('a') for x in coach.find_all('th')]\n",
    "        hrefs_coach = [None if x is None else x['href'] for x in hrefs_coach]\n",
    "\n",
    "        # Create Dataframe for Coaching Information from Team 'tcode' during season 'yr'\n",
    "        # {0:'Age',1:'Country',2:'DoB',3:'Role',4:'Start Date', 5: 'End Date'}\n",
    "        coach_info = pd.DataFrame(coach_info)\n",
    "        coach_info['Coach'] = coaching_staff\n",
    "        coach_info['URL'] = hrefs_coach\n",
    "        coach_info['TM'] = teamcode\n",
    "        coach_info[\"Season\"] = yr\n",
    "\n",
    "        global total_coach_info\n",
    "        total_coach_info = pd.concat([total_coach_info,coach_info],ignore_index=True)\n",
    "        \n",
    "        \n",
    "        #Player Convert Commented Table to Soup\n",
    "        player = BeautifulSoup(\n",
    "            [i for i in soup.find_all(string=lambda text: isinstance(text, Comment)) if 'id=\"appearances\"' in i][0]).find('tbody')\n",
    "\n",
    "        #Get Player Names\n",
    "        players = [x.get_text() for x in player.find_all('th')]\n",
    "\n",
    "        #Get Player Info Chart\n",
    "        player_info = np.array([x.get_text() for x in player.find_all('td')]).reshape(len(players),-1)\n",
    "\n",
    "        #Get Links for Each Player\n",
    "        hrefs_players = [x.find('a') for x in player.find_all('th')]\n",
    "        hrefs_players = [None if x is None else x['href'] for x in hrefs_players]\n",
    "\n",
    "        # Create Dataframe for Player Information from Team 'tcode' during season 'yr'\n",
    "        player_info = pd.DataFrame(player_info)\n",
    "        player_info['Player'] = players\n",
    "        player_info['URL'] = hrefs_players\n",
    "        player_info['TM'] = teamcode\n",
    "        player_info[\"Season\"] = yr\n",
    "        \n",
    "        global total_player_info\n",
    "        total_player_info = pd.concat([total_player_info,player_info],ignore_index=True)\n",
    "        \n",
    "        yr -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "018dcf03-4fb7-464d-b17d-bd6279ecfcf4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--no-default-browser-check')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--disable-extensions')\n",
    "options.add_argument('--disable-default-apps')\n",
    "\n",
    "ser = Service(\"C:\\\\Users\\\\finnr\\\\Downloads\\\\Zhao McIntire Research\\\\chromedriver_win32\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "id": "b7f758dc-5b30-4a50-b40e-41ff2f809b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\finnr\\AppData\\Local\\Temp\\ipykernel_17812\\2294712664.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),service=ser,options=options)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install(),service=ser,options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "651f7a73-2584-4ced-9d1e-0ff1cd3f6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_people_info_sel(teamcode,yr=2021):\n",
    "    \n",
    "    #starting information for link\n",
    "    url = f'https://www.baseball-reference.com/teams/{teamcode}/{yr}.shtml'\n",
    "    \n",
    "    while (yr != 1998):\n",
    "        \n",
    "        #Access Page\n",
    "        headers = {\"User-Agent\": ua.random,\n",
    "           \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "           \"Accept-Language\": \"en-US,en;q=0.7\",\n",
    "           \"Accept-Encoding\": \"gzip, deflate\",\n",
    "           \"Referer\": \"https://www.google.com/\",\n",
    "           \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
    "        \n",
    "        #Get Information From Page\n",
    "        global page\n",
    "        global soup\n",
    "        \n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        src = driver.page_source\n",
    "        time.sleep(3.1)\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        all_soup.append(soup)\n",
    "\n",
    "        url = 'https://www.baseball-reference.com' + soup.find('a',class_='button2 prev')['href']\n",
    "\n",
    "\n",
    "        #Coach Convert Commented Table to Soup\n",
    "        coach = BeautifulSoup(\n",
    "            [i for i in soup.find_all(string=lambda text: isinstance(text, Comment)) if 'id=\"coaches\"' in i][0]).find('tbody')\n",
    "\n",
    "        #Grab Coach Names\n",
    "        coaching_staff = [x.get_text() for x in coach.find_all('th')]\n",
    "\n",
    "        #Grab Coach Info Chart\n",
    "        coach_info = np.array([x.get_text() for x in coach.find_all('td')]).reshape(len(coaching_staff),-1)\n",
    "\n",
    "        #Get Links for Each Coach\n",
    "        hrefs_coach = [x.find('a') for x in coach.find_all('th')]\n",
    "        hrefs_coach = [None if x is None else x['href'] for x in hrefs_coach]\n",
    "\n",
    "        # Create Dataframe for Coaching Information from Team 'tcode' during season 'yr'\n",
    "        # {0:'Age',1:'Country',2:'DoB',3:'Role',4:'Start Date', 5: 'End Date'}\n",
    "        coach_info = pd.DataFrame(coach_info)\n",
    "        coach_info['Coach'] = coaching_staff\n",
    "        coach_info['URL'] = hrefs_coach\n",
    "        coach_info['TM'] = teamcode\n",
    "        coach_info[\"Season\"] = yr\n",
    "\n",
    "        global total_coach_info\n",
    "        total_coach_info = pd.concat([total_coach_info,coach_info],ignore_index=True)\n",
    "\n",
    "\n",
    "        #Player Convert Commented Table to Soup\n",
    "        player = BeautifulSoup(\n",
    "            [i for i in soup.find_all(string=lambda text: isinstance(text, Comment)) if 'id=\"appearances\"' in i][0]).find('tbody')\n",
    "\n",
    "        #Get Player Names\n",
    "        players = [x.get_text() for x in player.find_all('th')]\n",
    "\n",
    "        #Get Player Info Chart\n",
    "        player_info = np.array([x.get_text() for x in player.find_all('td')]).reshape(len(players),-1)\n",
    "\n",
    "        #Get Links for Each Player\n",
    "        hrefs_players = [x.find('a') for x in player.find_all('th')]\n",
    "        hrefs_players = [None if x is None else x['href'] for x in hrefs_players]\n",
    "\n",
    "        # Create Dataframe for Player Information from Team 'tcode' during season 'yr'\n",
    "        player_info = pd.DataFrame(player_info)\n",
    "        player_info['Player'] = players\n",
    "        player_info['URL'] = hrefs_players\n",
    "        player_info['TM'] = teamcode\n",
    "        player_info[\"Season\"] = yr\n",
    "\n",
    "        global total_player_info\n",
    "        total_player_info = pd.concat([total_player_info,player_info],ignore_index=True)\n",
    "\n",
    "        yr -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "afa68f98-9586-498a-a47a-49917a817b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_soup(soups,teamcode):\n",
    "    # prettify the soupfied html elements in the list\n",
    "    links_prettified = [s.prettify() for s in soups]\n",
    "\n",
    "    # add the custom word to standout, I chose BREAKHERE\n",
    "    list_with_word = [m+'BREAKHERE' for m in links_prettified]\n",
    "\n",
    "    # make one giant string, using the .join trick list_as_one = \"\".join(list_with_word)\n",
    "    # save the giant string to .txt with open('soup_list.txt', 'w') as file:   file.write(list_as_one)\n",
    "\n",
    "    # In order to know the team and season:\n",
    "    # title = soup.find_all('div',{'data-template':'Partials/Teams/Summary'})[0].find_all('span')\n",
    "    # title[0].get_text() +' ' + title[1].get_text()\n",
    "    \n",
    "    with open(f'baseballsoup{teamcode}.txt', 'w', encoding='utf-8') as f:\n",
    "        for soup in list_with_word:\n",
    "            f.write(soup)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "e9a36526-806f-4285-b240-0fe924aabeca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team:  TOR\n",
      "Team:  BAL\n",
      "Team:  CHW\n",
      "Team:  CLE\n",
      "Team:  DET\n",
      "Team:  KCR\n",
      "Team:  MIN\n",
      "Team:  HOU\n",
      "Team:  SEA\n",
      "Team:  OAK\n",
      "Team:  LAA\n",
      "Team:  TEX\n",
      "Team:  ATL\n",
      "Team:  PHI\n",
      "Team:  NYM\n",
      "Team:  MIA\n",
      "Team:  WSN\n",
      "Team:  MIL\n",
      "Team:  STL\n",
      "Team:  CIN\n",
      "Team:  CHC\n",
      "Team:  PIT\n",
      "Team:  SFG\n",
      "Team:  LAD\n",
      "Team:  SDP\n",
      "Team:  COL\n",
      "Team:  ARI\n"
     ]
    }
   ],
   "source": [
    "for n in np.arange(len(teamcodes)):\n",
    "    t = teamcodes[n]\n",
    "\n",
    "    print(\"Team: \", t)\n",
    "    \n",
    "    #Create Copy Before It Gets Manipulated\n",
    "    temp_total_player_info = total_player_info.copy()\n",
    "    temp_total_coach_info = total_coach_info.copy()\n",
    "\n",
    "    #Reset the Soups\n",
    "    all_soups = list()\n",
    "\n",
    "    #Get Info\n",
    "    get_people_info_sel(t)\n",
    "\n",
    "    #Put Soups into .txt File\n",
    "    big_soup(all_soup,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "7d000fde-fba9-448f-9c6f-dd26d0f0b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_player_info.to_csv('C:\\\\Users\\\\finnr\\\\Downloads\\\\Zhao McIntire Research\\\\Baseball\\\\teamrosters.csv')\n",
    "total_coach_info.to_csv('C:\\\\Users\\\\finnr\\\\Downloads\\\\Zhao McIntire Research\\\\Baseball\\\\coaches.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
